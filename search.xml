<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>我的第一篇博客文章</title>
    <url>/2023/01/17/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[<h3 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h3><p>从今天起，正式开始写博客啦。这次用hexo搭建博客还有许多要完善的地方。一些小的细节方面后续也会持续改进。最近其实还是比较忙的，这次也算是一个总结和记录吧（虽说也没有做啥总结）。总之，依然要做好安排，不断学习，不断进步。</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>Start</title>
    <url>/2023/01/08/Start/</url>
    <content><![CDATA[<h2 id="1月8日记录"><a href="#1月8日记录" class="headerlink" title="1月8日记录"></a>1月8日记录</h2><p>今天用hexo框架搭建了属于自己的博客，还是很开心的，虽然遇到了许多困难，但是我还是一步一步的实现了自己的博客。感觉很开心。也希望自己以后不断学习，不断提高自己的技术，在IT的旅途上追逐梦想，扬帆起航。</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构学习记录1</title>
    <url>/2023/01/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951/</url>
    <content><![CDATA[<h1 id="串和广义表"><a href="#串和广义表" class="headerlink" title="串和广义表"></a>串和广义表</h1><h3 id="串的定义和运算"><a href="#串的定义和运算" class="headerlink" title="串的定义和运算"></a>串的定义和运算</h3><p>1.串是由零个或任意多个字符组成的有限序列<br>2.子串是串中任意连续的字符组成的子序列<br>注：空串是任意串的子串，任意串是自身的子串<br>3.模式匹配是一种求子串在主串中第一次出现的第一个字符的位置<br>4.串的操作有很多种，比如赋值，复制，求串的长度，串的连接，插入删除等等<br>5.串的存储结构有顺序存储和链式存储<br>6.在实际应用中，可以采用堆分配存储（动态存储）<br>堆分配存储的方法：<br>（1）开辟一块地址连续的存储空间，该存储空间称为“堆”<br>（2）建立一个索引表，用来存储串的名字，长度和该串在堆中的起始位置<br>（3）程序执行过程中，每产生一个串，系统就从堆中分配一块大小和串长度相等的连续空间，用来存储该串的值，并且在索引表中增加一个索引项，用于记录串的信息。</p>
<h3 id="广义表"><a href="#广义表" class="headerlink" title="广义表"></a>广义表</h3><p>1.广义表是n个数据元素的有序序列，它是线性表的推广，也称为“列表”<br>2.广义表是一种多层次的数据结构，其中的元素可以是单元素，也可以是子表<br>3.广义表可以是递归的表，即它可以是自身的子表<br>4.广义表可以被其他表所共享<br>5.广义表常采用链式存储结构，有头尾表示法和孩子兄弟表示法两种存储方式</p>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构学习纪录2</title>
    <url>/2023/01/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0%E7%BA%AA%E5%BD%952/</url>
    <content><![CDATA[<h1 id="树形结构"><a href="#树形结构" class="headerlink" title="树形结构"></a>树形结构</h1><h3 id="树的定义："><a href="#树的定义：" class="headerlink" title="树的定义："></a>树的定义：</h3><p>1.树是n（n&gt;&#x3D;0）个有限数据元素的集合。</p>
<p>2.树满足的条件：</p>
<p>（1）有且仅有一个根节点</p>
<p>（2）其余的节点分为m（m&gt;&#x3D;0）个互不相交的非空集合，其中每个集合本身就是一棵树，称为根的子树</p>
<h3 id="树的表示方法："><a href="#树的表示方法：" class="headerlink" title="树的表示方法："></a>树的表示方法：</h3><p>1.树形表示法</p>
<p>2.嵌套集合表示法</p>
<p>3.凹入表示法</p>
<p>4.广义表示法</p>
<h3 id="树的基本术语："><a href="#树的基本术语：" class="headerlink" title="树的基本术语："></a>树的基本术语：</h3><p>1.结点：树的结点包含一个数据元素及若干指向其子树的分支</p>
<p>2.结点的度：结点所拥有的分支数目或后继结点个数</p>
<p>3.树的度：树中各结点度的最大值</p>
<p>4.叶子（终端结点）：度为零的结点称为叶子结点</p>
<p>5.分支（非终端结点）：度不为零的结点</p>
<p>6.孩子结点：一个结点的后继称为该结点的孩子结点</p>
<p>7.双亲结点：一个结点是其后继结点的双亲节点</p>
<p>8.兄弟结点：同一个双亲结点下的孩子节点互称为兄弟结点</p>
<p>9.堂兄弟：双亲互为兄弟的两个结点互称为堂兄弟</p>
<p>10.子孙结点：一个结点的所有子树的结点</p>
<p>11.祖先结点：从树根结点到达一个结点路径上的所有结点称为该结点的祖先结点</p>
<p>12.结点的层次：根结点的层次为1，其余结点的层次等于它双亲结点的层次加一</p>
<p>13.树的深度：树中结点的最大层次称为树的深度（或高度）</p>
<p>14.有序树、无序树：交换某结点各子树的相对位置，会构成不同的树，这样的树称为有序树，反之则是无序树</p>
<p>15.森林：m（m&gt;&#x3D;0）棵互不相交的树的集合</p>
<h3 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h3><p>1.二叉树是有n（n&gt;&#x3D;0）个结点的有限集合，该集合或者为空，或者由一个根结点及两个不相交的子树组成（左子树和右子树）</p>
<p>2.左子树和右子树同样都是二叉树</p>
<p>3.二叉树是特殊的有序树</p>
<p>4.二叉树的性质：</p>
<p>（1）在二叉树的第i层至多有2^(i-1)个结点</p>
<p>（2）深度为h的二叉树中至多有(2^h) - 1结点</p>
<p>（3）对任意一棵二叉树T，如果其叶子结点数为n0，度为2的结点数为n2，则有n0 &#x3D; n2 + 1</p>
<p>（4）具有n个结点的完全二叉树的深度为log2n（以2为底的对数） + 1</p>
<p>（5）如果一棵n个结点的完全二叉树的结点按层次有：</p>
<p>如果i&#x3D;1，结点i是根结点，无双亲；如果i&gt;1。则其双亲结点是结点i&#x2F;2</p>
<p>如果2i&gt;n，则结点i无左孩子，该结点为叶子结点；否则其左孩子是结点2i</p>
<p>如果2i+1&gt;n，则结点i无右孩子，该结点为叶子结点；否则其右孩子是结点2i+1</p>
<p>5.二叉树的存储结构：</p>
<p>（1）顺序存储</p>
<p>（2）链式存储</p>
<p>6.二叉树的遍历方式：</p>
<p>（1）先序遍历：根—-左—–右</p>
<p>（2）中序遍历：左—-根—-右</p>
<p>（3）后序遍历：左—-右—-根</p>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>算法学习记录1</title>
    <url>/2023/01/19/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951/</url>
    <content><![CDATA[<h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><h3 id="一、选择排序"><a href="#一、选择排序" class="headerlink" title="一、选择排序"></a>一、选择排序</h3><p>1.每一趟从待排序的数据元素中选择最小（或最大）的一个元素，顺序放在待排序的数组最前面，直到数据元素全部排完为止。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MAX = <span class="number">1001</span>;</span><br><span class="line"><span class="type">int</span> a[MAX];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> n,k,i,j;</span><br><span class="line">	<span class="type">int</span> temp;</span><br><span class="line">	cin&gt;&gt;n;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cin&gt;&gt;a[i];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="comment">//TODO</span></span><br><span class="line">		k = i;</span><br><span class="line">		<span class="keyword">for</span>(j = i+<span class="number">1</span>;j&lt;n;j++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">if</span>(a[j]&lt;a[k]) k = j;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span>(k!=i)</span><br><span class="line">		&#123;</span><br><span class="line">			temp = a[i];</span><br><span class="line">			a[i] = a[k];</span><br><span class="line">			a[k] = temp;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cout&lt;&lt;a[i]&lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="二、冒泡排序"><a href="#二、冒泡排序" class="headerlink" title="二、冒泡排序"></a>二、冒泡排序</h3><p>1.n个数据元素，从第一个开始，依次比较相邻的两个是否顺序，如果不满足就交换。直到n-1和n相比，经过n-1轮后，得到有序的队列</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MAX = <span class="number">10001</span>; </span><br><span class="line"><span class="type">int</span> a[MAX];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">bool</span> flag;</span><br><span class="line">	<span class="type">int</span> n,j;</span><br><span class="line">	cin&gt;&gt;n;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cin&gt;&gt;a[i];</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=n<span class="number">-1</span>;i&gt;=<span class="number">1</span>;i--)  <span class="comment">//n-1轮冒泡排序</span></span><br><span class="line">	&#123;</span><br><span class="line">		flag = <span class="literal">true</span>;</span><br><span class="line">		<span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;i;j++)  <span class="comment">//进行i次的比较</span></span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">if</span> (a[j]&gt;a[j+<span class="number">1</span>])</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="built_in">swap</span>(a[j],a[j+<span class="number">1</span>]);</span><br><span class="line">				flag = <span class="literal">false</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span>(flag)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">break</span>;  <span class="comment">//没有进行交换，直接退出循环</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cout&lt;&lt;a[i]&lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="三、插入排序"><a href="#三、插入排序" class="headerlink" title="三、插入排序"></a>三、插入排序</h3><p>1.当读入一个数据元素时，在已经排序好的序列中，寻找它正确的位置放入。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MAX=<span class="number">10001</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> a[MAX];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> n,i,j,k,temp;</span><br><span class="line">	cin&gt;&gt;n;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cin&gt;&gt;a[i];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span>(j=i<span class="number">-1</span>;j&gt;=<span class="number">0</span>;j--)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">if</span>(a[j]&lt;a[i]) <span class="keyword">break</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span>(j!=i<span class="number">-1</span>)&#123;</span><br><span class="line">			<span class="comment">//TODO</span></span><br><span class="line">			temp = a[i];</span><br><span class="line">			<span class="keyword">for</span>(k=i<span class="number">-1</span>;k&gt;j;k--)</span><br><span class="line">				a[k+<span class="number">1</span>] = a[k];</span><br><span class="line">			a[k+<span class="number">1</span>] = temp;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cout&lt;&lt;a[i]&lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="四、桶排序"><a href="#四、桶排序" class="headerlink" title="四、桶排序"></a>四、桶排序</h3><p>1.若待排序的值在一个明显有限的范围内，可设计有限个有序桶，待排序的值装入相对应的桶，顺序输出各桶的值就可以得到有序的序列。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> b[<span class="number">101</span>],n,i,k;</span><br><span class="line">	<span class="built_in">memset</span>(b,<span class="number">0</span>,<span class="built_in">sizeof</span>(b));</span><br><span class="line">	cin&gt;&gt;n;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cin&gt;&gt;k;</span><br><span class="line">		b[k]++;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;=<span class="number">100</span>;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">while</span>(b[i]&gt;<span class="number">0</span>)&#123;</span><br><span class="line">			<span class="comment">//TODO</span></span><br><span class="line">			cout&lt;&lt;i&lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">			b[i]--;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	cout&lt;&lt;endl;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="五、快速排序"><a href="#五、快速排序" class="headerlink" title="五、快速排序"></a>五、快速排序</h3><p>1.快速排序是对冒泡排序的一种改进。它的基本思想是：通过一趟排序将待排序的序列分为两部分，其中一部分中的每一个数都比另一部分小，则可以对这两部分继续进行排序，最终使得整个序列有序。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e6</span> + <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> q[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">quick_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(l&gt;=r) <span class="keyword">return</span>;</span><br><span class="line">	<span class="type">int</span> x = q[l],i = l<span class="number">-1</span>,j =r+<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">while</span>(i&lt;j)&#123;</span><br><span class="line">		<span class="comment">//TODO</span></span><br><span class="line">		<span class="keyword">do</span> i++;<span class="keyword">while</span>(q[i]&lt;x);</span><br><span class="line">		<span class="keyword">do</span> j--;<span class="keyword">while</span>(q[j]&gt;x);</span><br><span class="line">		<span class="keyword">if</span>(i&lt;j) <span class="built_in">swap</span>(q[i],q[j]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">quick_sort</span>(q,l,j);</span><br><span class="line">	<span class="built_in">quick_sort</span>(q,j+<span class="number">1</span>,r);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,&amp;n);</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>,&amp;q[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">quick_sort</span>(q,<span class="number">0</span>,n<span class="number">-1</span>);</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>,q[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h3 id="六、归并排序"><a href="#六、归并排序" class="headerlink" title="六、归并排序"></a>六、归并排序</h3><p>1.归并排序算法采用的是分治算法,即把两个(或两个以上)有序表合并成一个新的有序表,把待排序的序列分成若干个子序列,每个子序列都是有序的,然后把有序子序列合并成整体有序序列 。若将两个有序表合并成一个有序表，称为二路归并。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MAX = <span class="number">100001</span>;</span><br><span class="line"><span class="type">int</span> a[MAX];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Merge</span><span class="params">(<span class="type">int</span> arr[],<span class="type">int</span> low,<span class="type">int</span> mid,<span class="type">int</span> high)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> i = low,j = mid + <span class="number">1</span>,k = <span class="number">0</span>;</span><br><span class="line">	<span class="type">int</span> *temp = <span class="keyword">new</span> <span class="type">int</span> [high-low+<span class="number">1</span>];</span><br><span class="line">	<span class="keyword">while</span>(i&lt;=mid&amp;&amp;j&lt;=high)&#123;</span><br><span class="line">		<span class="comment">//TODO</span></span><br><span class="line">		<span class="keyword">if</span>(arr[i]&lt;=arr[j])</span><br><span class="line">		&#123;</span><br><span class="line">			temp[k++] = arr[i++]; <span class="comment">//较小的哪一个先存入temp数组中</span></span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">		&#123;</span><br><span class="line">			temp[k++] = arr[j++];</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">while</span>(i&lt;=mid)&#123;</span><br><span class="line">		<span class="comment">//TODO</span></span><br><span class="line">		temp[k++] = arr[i++];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">while</span>(j&lt;=high)&#123;</span><br><span class="line">		<span class="comment">//TODO</span></span><br><span class="line">		temp[k++] = arr[j++];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span>(i=low,k=<span class="number">0</span>;i&lt;=high;i++,k++)</span><br><span class="line">		arr[i] = temp[k];</span><br><span class="line">	<span class="keyword">delete</span> []temp; <span class="comment">//释放数组空间，必须用delete[]</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">MergeSort</span><span class="params">(<span class="type">int</span> arr[],<span class="type">int</span> low,<span class="type">int</span> high)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(low &gt;= high)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	&#123;</span><br><span class="line">		<span class="type">int</span> mid = (low+high)/<span class="number">2</span>;</span><br><span class="line">		<span class="built_in">MergeSort</span>(arr,low,mid);</span><br><span class="line">		<span class="built_in">MergeSort</span>(arr,mid+<span class="number">1</span>,high);</span><br><span class="line">		<span class="built_in">Merge</span>(arr,low,mid,high);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> n;</span><br><span class="line">	cin&gt;&gt;n;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cin&gt;&gt;a[i];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">MergeSort</span>(a,<span class="number">0</span>,n<span class="number">-1</span>);</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		cout&lt;&lt;a[i]&lt;&lt;<span class="string">&quot; &quot;</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构学习记录3</title>
    <url>/2023/01/19/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%953/</url>
    <content><![CDATA[<h1 id="图结构"><a href="#图结构" class="headerlink" title="图结构"></a>图结构</h1><h3 id="图的定义："><a href="#图的定义：" class="headerlink" title="图的定义："></a>图的定义：</h3><p>1.图是一个由非空的顶点集合和一个描述顶点之间的关系即边（Edges）的有限集合组成的一种数据结构。</p>
<p>可以定义为一个二元组：</p>
<p>G&#x3D;（V,E）</p>
<p>V是顶点集合，E是边的集合</p>
<p>2.按照图中的边是否有方向，图分为有向图和无向图两类</p>
<h3 id="图的相关术语："><a href="#图的相关术语：" class="headerlink" title="图的相关术语："></a>图的相关术语：</h3><p>（1）无向图：在一个图中，每条边都没有方向</p>
<p>（2）有向图：在一个图中，每条边都有方向</p>
<p>（3）无向完全图：在一个无向图中，任意两个顶点都有一条直接边相连。在一个含有n个顶点的无向完全图中，有（n-1）n&#x2F;2 条边</p>
<p>（4）有向完全图：在一个有向图中，任意两顶点之间都有方向互为相反的两条弧相连接</p>
<p>（5）顶点的度：一个顶点所拥有的边数</p>
<p>（6）顶点的入度：在有向图中，一个顶点拥有的弧头的数目</p>
<p>（7）顶点的出度：在有向图中，一个顶点拥有的弧尾的数目</p>
<p>一个顶点的度 &#x3D;  顶点的入度 + 顶点的出度</p>
<p>（8）权：图的边或弧有时具有与它相关的数据信息，这个数据信息称为权（权重）</p>
<p>（9）网：带权的图称为网，同样可以分为有向网和无向网</p>
<p>（10）回路或环：在一个路径中，若其第一个顶点和最后一个顶点是相同的，则称该路径为一个回路或环</p>
<p>（11）简单路径：表示路径的顶点序列中的顶点各不相同</p>
<p>（12）简单回路：除了第一个和最后一个顶点之外，其余各顶点均不重复出现</p>
<p>（13）稀疏图：有很少条边的图称为稀疏图</p>
<p>（14）连通图：无向图中，任意两个顶点都是连通的图称为连通图。无向图的极大连通子图称为连通分量</p>
<p>（15）强连通图：对于有向图来说，图中任意一对顶点Vi和Vj均有从一个顶点Vi到另一个顶点Vj有路径，也有从Vj到Vi的路径，则称该有向图是强连通图。有向图的极大强连通子图称为强连通分量</p>
<p>（16）生成树：连通图G的一个子图如果是一棵包含G的所有顶点的树，则该子图称为G的生成树</p>
<h3 id="图的存储结构："><a href="#图的存储结构：" class="headerlink" title="图的存储结构："></a>图的存储结构：</h3><p>（1）邻接矩阵</p>
<p>（2）邻接表</p>
<h3 id="图的遍历："><a href="#图的遍历：" class="headerlink" title="图的遍历："></a>图的遍历：</h3><p>1.深度优先搜索（DFS）</p>
<p>2.广度优先搜索（BFS）</p>
]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>算法学习记录2</title>
    <url>/2023/01/20/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952/</url>
    <content><![CDATA[<h1 id="搜索与回溯"><a href="#搜索与回溯" class="headerlink" title="搜索与回溯"></a>搜索与回溯</h1><h3 id="深度优先搜索（DFS）"><a href="#深度优先搜索（DFS）" class="headerlink" title="深度优先搜索（DFS）"></a>深度优先搜索（DFS）</h3><p>1.深度优先搜索类似于树的先序遍历，是树的先序遍历的推广</p>
<p>2.深度优先搜索方法：</p>
<p>（1）首先从图中某个顶点v出发，首先访问此顶点，将其标记为已经访问</p>
<p>（2）任选一个v的未被访问的邻接点w出发，继续进行深度优先搜索</p>
<p>（3）直到图中所有和v路径想通的顶点被访问到</p>
<p>（4）若此时还有顶点未访问，则选另一个顶点作为起始点，重复上述步骤，直到所有的顶点都被访问为止。</p>
<h3 id="广度优先搜索（BFS）"><a href="#广度优先搜索（BFS）" class="headerlink" title="广度优先搜索（BFS）"></a>广度优先搜索（BFS）</h3><ol>
<li>广度优先搜索类似于树的按层次遍历</li>
<li>广度优先搜索的思想：</li>
</ol>
<p>（1）从图中的某一个顶点V0开始，访问V0，</p>
<p>（2）访问与V0相邻接的顶点V1，V2，……Vt</p>
<p>（3）依次访问与V1，V2，……..Vt相邻接的点</p>
<p>（4）循环进行，直到所有的顶点都被访问</p>
<h3 id="回溯"><a href="#回溯" class="headerlink" title="回溯"></a>回溯</h3><p>1.回溯法是从初始状出发，按照深度优先搜索的方式，根据产生子结点的条件约束，搜索问题的解。当发现当前节点满足不了求解条件时，就回溯，尝试其他的路径。</p>
<p>注：在搜索中，常采用剪枝的策略进行优化</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>算法学习记录3</title>
    <url>/2023/01/21/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%953/</url>
    <content><![CDATA[<h1 id="贪心和分治"><a href="#贪心和分治" class="headerlink" title="贪心和分治"></a>贪心和分治</h1><h3 id="一、贪心算法"><a href="#一、贪心算法" class="headerlink" title="一、贪心算法"></a>一、贪心算法</h3><p>1.选择贪心策略，根据贪心策略，一步一步地得到局部最优解，最后把所有的局部最优解合成原来问题的一个最优解。</p>
<p>2.基本思路：</p>
<p>（1）建立数学模型来描述问题</p>
<p>（2）把求解的问题分成若干个子问题</p>
<p>（3）对每一个子问题求解，得到子问题的局部最优解</p>
<p>（4）把子问题的局部最优解合成原来问题的一个解</p>
<p>3.适用的问题：</p>
<p>局部最优的策略能导致产生全局最优解</p>
<h3 id="二、分治算法"><a href="#二、分治算法" class="headerlink" title="二、分治算法"></a>二、分治算法</h3><p>1.所谓分治，就是分而治之，将大规模的问题分解成几个小规模的问题。通过求解小规模的问题来求出大规模问题的解。</p>
<p>2.当我们将问题分解成两个较小问题求解时的分治称为二分法</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>算法学习记录4</title>
    <url>/2023/01/22/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%954/</url>
    <content><![CDATA[<h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><p>1.动态规划是解最优化问题的一种途径，一种方法。</p>
<p>2.多阶段决策问题：</p>
<p>一个问题可以看作是一个前后关联具有链状结构的多阶段过程。</p>
<p>3.多阶段决策过程是指一类特殊的活动过程，问题可以按时间顺序分解成若干相互联系的阶段，在每一个阶段都要作出决策，全部过程的决策是一个决策序列。</p>
<p>4.相关的概念：</p>
<p>（1）阶段和阶段变量</p>
<p>（2）状态和状态变量</p>
<p>（3）决策、决策变量和决策允许集合</p>
<p>（4）策略和最优策略</p>
<p>（5）状态转移方程</p>
<p>5.能够用动态规划解决的问题必须满足的条件：</p>
<p>最优化原理和无后效性原则</p>
<p>6.解题步骤：</p>
<p>（1）阶段</p>
<p>（2）状态</p>
<p>（3）决策</p>
<p>（4）策略</p>
<p>（5）状态转移方程</p>
<p>7.注意事项：</p>
<p>（1）确定好dp数组以及下标的含义</p>
<p>（2）确定递推公式</p>
<p>（3）dp数组的初始化</p>
<p>（4）确定遍历顺序</p>
<p>（5）推导dp数组</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统学习记录1</title>
    <url>/2023/01/22/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951/</url>
    <content><![CDATA[<h1 id="计算机系统"><a href="#计算机系统" class="headerlink" title="计算机系统"></a>计算机系统</h1><p>1.计算机系统：包含硬件系统和软件系统</p>
<p>2.硬件：借助电、磁、光、机械等原理构成的各种物理部件的有机组合，是系统工作的实体</p>
<p>主要有CPU，主存储器，I&#x2F;O控制系统，外围设备</p>
<p>（1）中央处理器：包含运算单元和控制单元</p>
<p>（2）外围设备：输入、输出设备</p>
<p>（3）总线（BUS）是计算机各种功能部件之间传送信息的公共通信干线，它是CPU、内存、输入输出设备传递信息的公用通道。包括控制总线、地址总线、数据总线</p>
<p>计算机的各个部件通过总线相连接</p>
<p>总线的类型：</p>
<p>内部总线</p>
<p>系统总线</p>
<p>通信总线</p>
<p>（4）CPU：中央处理器是计算机的运算核心，主要包括运算逻辑单元，寄存器部件，控制部件</p>
<p>（5）存储器：</p>
<p>cache：高速缓存</p>
<p>SARM：静态随机存储器</p>
<p>DARM：动态随机存储器（主存）</p>
<p>3.软件：各种程序和文件，用于指挥计算机系统按指定的要求进行协同工作</p>
<p>包括系统软件、支撑软件和应用软件</p>
<p>关键的系统软件是：操作系统和语言处理系统</p>
<p>（1）系统软件：</p>
<p>操作系统、实用程序、语言处理程序、数据库管理程序</p>
<p>（2）支撑软件：</p>
<p>接口软件、工具软件、软件数据库、</p>
<p>（3）应用软件：</p>
<p>用户按其需要自行编写的专用程序</p>
]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录1</title>
    <url>/2023/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951/</url>
    <content><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h3 id="一、评估方法"><a href="#一、评估方法" class="headerlink" title="一、评估方法"></a>一、评估方法</h3><p>（获得测试结果）</p>
<h4 id="测试集的获取："><a href="#测试集的获取：" class="headerlink" title="测试集的获取："></a>测试集的获取：</h4><p>1.测试集和训练集应该互斥</p>
<p>2.常见的获取测试集的方法：</p>
<p>（1）留出法：</p>
<p>保持数据的一致性，如分层采样；多次重复划分；测试集不能太大，也不能太小</p>
<p>（2）交叉验证法</p>
<p>k折交叉验证</p>
<p>（3）自助法：自助采样</p>
<h3 id="二、调参与验证集"><a href="#二、调参与验证集" class="headerlink" title="二、调参与验证集"></a>二、调参与验证集</h3><h4 id="调参与最终模型："><a href="#调参与最终模型：" class="headerlink" title="调参与最终模型："></a>调参与最终模型：</h4><p>1.算法的参数：一般由人工设定，亦称为“超参数”</p>
<p>2.模型的参数：一般由学习确定</p>
<p>3.调参过程相似：先产生若干模型，然后基于某种评估方法进行选择</p>
<p>4.验证集：训练集中专门用来调参数的过程</p>
<h3 id="三、性能度量"><a href="#三、性能度量" class="headerlink" title="三、性能度量"></a>三、性能度量</h3><p>（评估性能的优劣）</p>
<p>1.性能度量是衡量模型泛化能力的评估标准，反映了任务需求。使用不同的性能度量往往会导致不同的评估结果。</p>
<p>2.什么样的模型是“好”的，不仅取决于算法和数据，还取决于任务需求。</p>
<p>3.回归任务常采用均方误差</p>
<p>4.对于分类任务，可以得到混淆矩阵。根据混淆矩阵，得到查准率（P）和查全率（R），进而得到F1度量<br><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -1.6ex;" xmlns="http://www.w3.org/2000/svg" width="20.533ex" height="4.636ex" role="img" focusable="false" viewBox="0 -1342 9075.4 2049"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(594.5,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mn" transform="translate(749,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><rect width="1449" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(1966.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(3022.6,0)"><g data-mml-node="mn" transform="translate(220,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220,-686)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="700" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(4184.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(4685,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mfrac" transform="translate(5074,0)"><g data-mml-node="mn" transform="translate(345.5,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><rect width="951" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(6487.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mfrac" transform="translate(7487.4,0)"><g data-mml-node="mn" transform="translate(349.5,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><rect width="959" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(8686.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
<h3 id="四、比较检验"><a href="#四、比较检验" class="headerlink" title="四、比较检验"></a>四、比较检验</h3><p>（判断实质差别）</p>
<p>1.在某种度量下取得评估结果后，不能直接比较以评判优劣</p>
<p>2.t检验</p>
<p>3.卡方检验</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录2</title>
    <url>/2023/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952/</url>
    <content><![CDATA[<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h4 id="一、线性回归"><a href="#一、线性回归" class="headerlink" title="一、线性回归"></a>一、线性回归</h4><p>1.线性回归要把离散转换为连续</p>
<p>2.离散属性的处理：</p>
<p>若有序，则连续化；否则，转换为k维向量</p>
<p>3.使用最小二乘法求解目标函数的最小值</p>
<h4 id="二、多元线性回归"><a href="#二、多元线性回归" class="headerlink" title="二、多元线性回归"></a>二、多元线性回归</h4><p>1.引入正则化（归纳偏好）</p>
<h4 id="三、广义线性模型"><a href="#三、广义线性模型" class="headerlink" title="三、广义线性模型"></a>三、广义线性模型</h4><p>1.对数线性回归，用线性回归来逼近对数的目标</p>
<h4 id="四、对率回归（logistic-regression）"><a href="#四、对率回归（logistic-regression）" class="headerlink" title="四、对率回归（logistic regression）"></a>四、对率回归（logistic regression）</h4><p>1.二分类任务，使用对率函数</p>
<p>2.对率回归的优点：</p>
<p>（1）无需事先假设数据分布</p>
<p>（2）可得到“类别”的近似概率预测</p>
<p>（3）可直接应用现有的数值优化算法求取最优解</p>
<p>3.对率回归是分类学习算法</p>
<h4 id="五、类别不平衡"><a href="#五、类别不平衡" class="headerlink" title="五、类别不平衡"></a>五、类别不平衡</h4><p>1.不同类别的样本比例相差很大，“小类”往往更重要</p>
<p>2.常用的类别不平衡学习方法：</p>
<p>（1）过采样</p>
<p>（2）欠采样</p>
<p>（3）阈值移动</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录3</title>
    <url>/2023/01/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%953/</url>
    <content><![CDATA[<h1 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h1><p>之前只是听说过xgboost这个模型，但是没有具体的学习。这次的学习总结就来具体的认识xgboost，这里没有深入的研究其中的数学原理，仅仅是从初学者的角度来认识，一些地方存在错误是在所难免的，希望大家多多指正交流。</p>
<p>1.Xgboost是一个监督模型，其对应于特殊的决策树——分类回归树（CART树）</p>
<p>2.用CART树做预测：将各个树的预测分数相加</p>
<p>3.对于监督学习来说，建立模型后的参数调整是非常重要的，知识藏在这些参数之中</p>
<p>4.xgboost的目标函数包含了损失函数和正则项</p>
<p>5.xgboost模型由CART树组成，参数存在于每棵CART树中</p>
<p>6.xgboost用GBDT（梯度提升树）算法分步骤优化目标函数，先优化第一棵树，在优化第二棵，直到第k棵</p>
<p>7.由官方的文档我们可以得知，xgboost代表极端梯度提升</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录4</title>
    <url>/2023/01/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%954/</url>
    <content><![CDATA[<h1 id="Sklearn"><a href="#Sklearn" class="headerlink" title="Sklearn"></a>Sklearn</h1><h3 id="一、基本概念："><a href="#一、基本概念：" class="headerlink" title="一、基本概念："></a>一、基本概念：</h3><p>1.sklearn是一个开源的python机器学习算法库</p>
<h3 id="二、-线性分类"><a href="#二、-线性分类" class="headerlink" title="二、 线性分类"></a>二、 线性分类</h3><p>1.鸢尾花数据的分类：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X_iris,y_iris = iris.data,iris.target</span><br><span class="line">X,y = X_iris[:, :<span class="number">2</span>],y_iris</span><br><span class="line"><span class="comment"># print(X.shape)</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"><span class="built_in">print</span>(X_train.shape,y_train.shape)</span><br><span class="line"></span><br><span class="line">scaler = preprocessing.StandardScaler().fit(X_train)</span><br><span class="line">X_train = scaler.transform(X_train)</span><br><span class="line">X_test  = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line">colors = [<span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;greenyellow&#x27;</span>,<span class="string">&#x27;blue&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(colors)):</span><br><span class="line">	xs = X_train[:,<span class="number">0</span>][y_train == i]</span><br><span class="line">	ys = X_train[:,<span class="number">1</span>][y_train == i]</span><br><span class="line">	plt.scatter(xs,ys,c=colors[i])</span><br><span class="line">plt.legend(iris.target_names)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Sepal length&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sepal width&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 山鸢尾setosa</span></span><br><span class="line"><span class="comment"># 杂色鸢尾vcersicolor</span></span><br><span class="line"><span class="comment"># 弗吉尼亚鸢尾 virginca0</span></span><br><span class="line"><span class="comment"># 上面的模型对山鸢尾的分类效果很好，但是不能将杂色鸢尾和弗吉尼亚鸢尾很好的分开</span></span><br></pre></td></tr></table></figure></div>

<p>2.在线性模型中使用梯度下降法：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X_iris,y_iris = iris.data,iris.target</span><br><span class="line">X,y = X_iris[:, :<span class="number">2</span>],y_iris</span><br><span class="line"><span class="comment"># print(X.shape)</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"><span class="built_in">print</span>(X_train.shape,y_train.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clf = SGDClassifier()</span><br><span class="line">clf.fit(X_train,y_train)</span><br><span class="line"><span class="built_in">print</span>(clf.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn中的三种流行的评估函数</span></span><br><span class="line"><span class="comment"># 精确率</span></span><br><span class="line"><span class="comment"># 召回率</span></span><br><span class="line"><span class="comment"># F1度量</span></span><br><span class="line">y_train_pred = clf.predict(X_train)</span><br><span class="line"><span class="built_in">print</span>(metrics.accuracy_score(y_train,y_train_pred))</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(metrics.accuracy_score(y_test,y_pred))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(metrics.classification_report(y_test,y_pred,target_names=iris.target_names))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;----------------------------------&quot;</span>)</span><br><span class="line"><span class="comment"># 打印混淆矩阵</span></span><br><span class="line"><span class="built_in">print</span>(metrics.confusion_matrix(y_test,y_pred))</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h3 id="二、支持向量机和图像识别"><a href="#二、支持向量机和图像识别" class="headerlink" title="二、支持向量机和图像识别"></a>二、支持向量机和图像识别</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_olivetti_faces</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score,KFold</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> sem</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">faces = fetch_olivetti_faces()</span><br><span class="line"><span class="built_in">print</span>(faces.DESCR)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(faces.keys())</span><br><span class="line"></span><br><span class="line">svc_1 = SVC(kernel=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(faces.data,faces.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evalute_cross_validation</span>(<span class="params">clf,X,y,K</span>)</span><br><span class="line">    cv = KFold(<span class="built_in">len</span>(y),K,shuffle=<span class="literal">True</span>,random_state=<span class="number">0</span>)</span><br><span class="line">    scores = cross_val_score(clf,X,y,cv=cv)</span><br><span class="line">    <span class="built_in">print</span>(scores)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Mean score:&#123;0:.3f&#125;(+/-&#123;1:.3f&#125;))&quot;</span>.<span class="built_in">format</span>(np.mean(scores),sem(scores))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">evalute_cross_validation(svc_1,X_train,y_train,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_evaluate</span>(<span class="params">clf,X_train,X_test,y_train,y_test</span>):</span><br><span class="line">    clf.fit(X_train,y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(clf.score(X_train,y_train))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy on testing set:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(clf.score(X_test,y_test))</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Classification Report:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(metrics.classification_report(y_test,y_pred))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(metrics.confusion_matrix(y_test,y_pred))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_and_evaluate(svc_1,X_train,X_test,y_train,y_test)</span><br></pre></td></tr></table></figure></div>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录5</title>
    <url>/2023/01/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%955/</url>
    <content><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h3 id="一、决策树模型："><a href="#一、决策树模型：" class="headerlink" title="一、决策树模型："></a>一、决策树模型：</h3><p>1.决策树基于“树”的结构进行决策</p>
<p>每个“内部”结点对应某个属性上的“测试”</p>
<p>每个分支对应于该测试的一种可能结果（即该属性的某个取值）</p>
<p>每个“叶结点”对应于一个“预测结果”</p>
<p>2.学习过程：</p>
<p>通过对训练样本的分析来确定“划分属性”（即内部结点所对应的属性）</p>
<p>3.预测过程：</p>
<p>将测试实例从根结点开始，沿着划分属性所构成的“判定测试序列”下行，直到叶结点	</p>
<p>4.三种停止的条件：</p>
<p>（1）当前结点包含的样本全属于同一类别，无需划分；</p>
<p>（2）当前属性集为空，或是所有样本在所有属性上的取值相同，无需划分</p>
<p>（3）当前结点所包含的样本集合为空，不能划分</p>
<h3 id="二、信息增益的划分："><a href="#二、信息增益的划分：" class="headerlink" title="二、信息增益的划分："></a>二、信息增益的划分：</h3><p>1.信息增益直接以信息熵为基础，计算当前划分对信息熵所造成的影响（ID3算法使用）</p>
<h3 id="三、其他属性的划分："><a href="#三、其他属性的划分：" class="headerlink" title="三、其他属性的划分："></a>三、其他属性的划分：</h3><p>1.增益率</p>
<p>2.启发式：先从候选划分属性中找到信息增益高于平均水平的，再从中选取增益率最高的（C4.5算法使用）</p>
<p>3.基尼系数</p>
<p>在候选属性集合中，选取那个使划分后基尼系数最小的属性（CART算法使用）</p>
<h3 id="四、决策树的剪枝："><a href="#四、决策树的剪枝：" class="headerlink" title="四、决策树的剪枝："></a>四、决策树的剪枝：</h3><p>1.剪枝是决策树用来防止过拟合的主要手段</p>
<p>2.基本策略：</p>
<p>预剪枝</p>
<p>后剪枝</p>
<p>3.在使用单个决策树的时候，一定要考虑剪枝</p>
<h3 id="五、缺失值的处理："><a href="#五、缺失值的处理：" class="headerlink" title="五、缺失值的处理："></a>五、缺失值的处理：</h3><p>1.样本赋权，权重划分</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>java学习记录1</title>
    <url>/2023/01/28/java%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951/</url>
    <content><![CDATA[<h1 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h1><h3 id="一、面向对象的概念："><a href="#一、面向对象的概念：" class="headerlink" title="一、面向对象的概念："></a>一、面向对象的概念：</h3><p>1.在面向对象（oop）中，程序被看做相互协作的对象集合，每个对象都是某个类的实例，所有类构成一个通过继承关系相联系的层次结构。</p>
<p>2.面向对象的特性：</p>
<p>封装、继承、多态</p>
<h3 id="二、类和对象："><a href="#二、类和对象：" class="headerlink" title="二、类和对象："></a>二、类和对象：</h3><p>1.在java中使用class关键字来定义类，使用new关键字可以实例化一个对象。</p>
<p>2.可以在类体中声明两种类的成员：成员变量和成员方法</p>
<p>成员变量是类的属性</p>
<p>3.一个方法最多只能有一个变长参数</p>
<p>4.this关键字指向的是当前对象的引用 </p>
<p>5.系统常用的类：</p>
<p>Date类、GregorianCalendar类</p>
<h3 id="三、访问控制符："><a href="#三、访问控制符：" class="headerlink" title="三、访问控制符："></a>三、访问控制符：</h3><p>1.java中有公共的（public）、私有的（private）、保护的（protected）和默认的（default）四种访问控制符</p>
<p>public：所定义的类中的成员变量或方法能够被其他包中的类所访问</p>
<p>default：什么访问控制符都不加，就是默认访问级别，表示一个类的资源仅允许在包内访问，同一子类不能访问</p>
<p>protected：定义为保护级别的成员除了可以在同一类中被访问外，还可以被同一包中的类和子类访问，同一子类可以访问</p>
<p>private：凡是private声明的成员变量或方法只能在类内访问</p>
<p>2.static关键字表示“全局”、“静态”的概念。被static修饰的成员变量和成员方法独立于该类的任何对象，不依赖类的特定实例，被类的所有实例共享。</p>
<p>static修饰的成员变量和成员方法习惯上被称为静态变量和静态方法，可以直接通过类名来访问</p>
<p>3.final关键字表示“不可变”</p>
<p>final成员变量在对象生成时只初始化一次</p>
<p>4.package的使用：</p>
<p>package是类（接口）的集合</p>
<p>包名一般为小写，而类名的第一个字母一般为大写</p>
<p>使用import关键字导入包</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录6</title>
    <url>/2023/01/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%956/</url>
    <content><![CDATA[<h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h3 id="一、支持向量机基本型："><a href="#一、支持向量机基本型：" class="headerlink" title="一、支持向量机基本型："></a>一、支持向量机基本型：</h3><p>1.线性分类器：正中间的鲁棒性好，泛化能力强</p>
<p>2.间隔与支持向量</p>
<p>3.寻找最大间隔（凸优化）</p>
<p>4.寻找一个线性可分的超平面</p>
<h3 id="二、对偶问题与解的特性："><a href="#二、对偶问题与解的特性：" class="headerlink" title="二、对偶问题与解的特性："></a>二、对偶问题与解的特性：</h3><p>1.使用拉格朗日乘子法</p>
<p>2.解必须满足KKT条件</p>
<p>3.解的稀疏性：训练完成后，最终模型仅与支持向量有关</p>
<h3 id="三、求解方法："><a href="#三、求解方法：" class="headerlink" title="三、求解方法："></a>三、求解方法：</h3><p>1.求解方法：SMO（求解凸优化问题）</p>
<p>2.通常使用所有支持向量求解的平均值</p>
<h3 id="四、特征空间的映射："><a href="#四、特征空间的映射：" class="headerlink" title="四、特征空间的映射："></a>四、特征空间的映射：</h3><p>1.如果原始空间是有限维（属性数有限），那么一定存在一个高维特征空间使样本线性可分</p>
<h3 id="五、核函数（kernel-function）："><a href="#五、核函数（kernel-function）：" class="headerlink" title="五、核函数（kernel function）："></a>五、核函数（kernel function）：</h3><p>1.绕过显式考虑特征映射，以及计算高维内积的困难</p>
<p>2.Mercer定理：若一个对称函数所对应的核矩阵半正定，则它就能作为核函数使用</p>
<p>3.任何一个核函数，都隐式地定义了一个RKHS（再生核希尔伯特空间）</p>
<p>4.核函数选择成为了决定支持向量机性能的关键</p>
<h3 id="六、如何使用SVM："><a href="#六、如何使用SVM：" class="headerlink" title="六、如何使用SVM："></a>六、如何使用SVM：</h3><p>1.以回归学习为例：</p>
<p>允许模型输出与实际输出之间存在2ε的差别</p>
<p>2.支持向量回归使用的ε-不敏感损失函数</p>
<p>3.支持向量回归（SVR）</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>java学习记录2</title>
    <url>/2023/01/29/java%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952/</url>
    <content><![CDATA[<h1 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h1><h3 id="一、什么是继承："><a href="#一、什么是继承：" class="headerlink" title="一、什么是继承："></a>一、什么是继承：</h3><p>1.面向对象类中存在以下常见的关系：</p>
<p>USES-A关系，类A用到了类B</p>
<p>HAS-A关系，类A中有类B的成员引用变量，则类A拥有类B</p>
<p>IS-A关系，继承关系，一个类是另一个类的一种</p>
<p>2.被继承的类一般称为“超类”或“父类”，继承的类被称为“子类”</p>
<p>3.使用extends关键字来描述继承</p>
<p>4.一个类可以同时被任意多个类继承</p>
<h3 id="二、继承与隐藏："><a href="#二、继承与隐藏：" class="headerlink" title="二、继承与隐藏："></a>二、继承与隐藏：</h3><p>1.当成员变量声明为private类型时，任何子类均不能继承该成员</p>
<p>2.当成员变量被修饰为protected类型时，若访问该变量的类位于包外，则只有通过继承才能访问该变量</p>
<h3 id="三、方法的重写："><a href="#三、方法的重写：" class="headerlink" title="三、方法的重写："></a>三、方法的重写：</h3><p>1.在子类自身的方法中，若与继承过来的方法具有相同的签名，便构成了方法的重写</p>
<p>2.如果子类重写了方法，则调用子类重写的方法，否则将调用从父类继承的方法</p>
<p>3.重写是基于继承的，如果不能继承一个方法，则不能构成重写，也就不必遵循重写规则</p>
<p>4.可以使用super调用父类被重写的方法</p>
<h3 id="四、方法的重载："><a href="#四、方法的重载：" class="headerlink" title="四、方法的重载："></a>四、方法的重载：</h3><p>1.方法重载是指在同一个类里面，有两个使用或两个以上具有相同名称、不同参数序列的方法</p>
<h3 id="五、final与继承："><a href="#五、final与继承：" class="headerlink" title="五、final与继承："></a>五、final与继承：</h3><p>1.任何其他类都不能继承用final修饰的类</p>
<p>2.什么时候使用final修饰类：需要确保类中的所有方法都不要被重写改进</p>
<h3 id="六、抽象类："><a href="#六、抽象类：" class="headerlink" title="六、抽象类："></a>六、抽象类：</h3><p>1.把很多类具有相同特征的事物归为一个抽象类</p>
<p>2.使用abstract关键字声明抽象类</p>
<p>3.抽象类不能实例化，即不能创建对象</p>
<p>4.当某类继承自抽象类时，如果其本身不是抽象类，则必须实现所继承抽象类中的抽象方法。</p>
<p>也就是说抽象类的第一个非抽象子类必须实现其父类所有的抽象方法，其中也包括父类继承的抽象方法</p>
<p>5.方法永远不可能同时标识为abstract和final，其之间存在着相反的含义</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习记录1</title>
    <url>/2023/01/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951/</url>
    <content><![CDATA[<h1 id="Numpy基础"><a href="#Numpy基础" class="headerlink" title="Numpy基础"></a>Numpy基础</h1><h4 id="一、生成numpy数组："><a href="#一、生成numpy数组：" class="headerlink" title="一、生成numpy数组："></a>一、生成numpy数组：</h4><p>1.numpy封装了一个新的数据类型ndarray，它是一个多维数组对象</p>
<p>2.可以直接对列表、元组等进行转换来生成ndarray</p>
<p>3.使用random模块生成数组：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">np.random.random   <span class="comment"># 生成0到1之间的随机数</span></span><br><span class="line">np.random.uniform  <span class="comment"># 生成均匀分布的随机数</span></span><br><span class="line">np.random.randn    <span class="comment"># 生成标准正态的随机数</span></span><br><span class="line">np.random.randint  <span class="comment"># 生成随机的整数</span></span><br><span class="line">np.random.normal   <span class="comment"># 生成正态分布</span></span><br><span class="line">np.random.shuffle  <span class="comment"># 随机打乱顺序</span></span><br><span class="line">np.random.seed     <span class="comment"># 设置随机种子</span></span><br><span class="line">random_sample      <span class="comment"># 生成随机的浮点数</span></span><br></pre></td></tr></table></figure></div>

<p>4.创建特定形状的多维数组：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">np.zeros((<span class="number">3</span>,<span class="number">4</span>))    <span class="comment"># 创建3*4的元素全为0的数组</span></span><br><span class="line">np.ones((<span class="number">3</span>,<span class="number">4</span>))     <span class="comment"># 创建3*4的元素全为1的数组</span></span><br><span class="line">np.empty((<span class="number">2</span>,<span class="number">3</span>))    <span class="comment"># 创建2*3的空数组，数据的值并不为0，而是没有初始化的垃圾值</span></span><br><span class="line">np.zero_like(ndarr)<span class="comment"># 以ndarr相同维度创建元素全为0数组</span></span><br><span class="line">np.ones_like(ndarr)<span class="comment"># 以ndarr相同维度创建元素全为1数组</span></span><br><span class="line">np.empty_like(ndarr)<span class="comment"># 以ndarr相同维度创建空数组</span></span><br><span class="line">np.eye(<span class="number">5</span>)          <span class="comment"># 该函数用来创建一个5*5的矩阵，对角线元素为1，其余元素都为0</span></span><br><span class="line">np.full((<span class="number">3</span>,<span class="number">5</span>),<span class="number">666</span>) <span class="comment"># 创建3*5的元素全为666数组，666为指定值</span></span><br></pre></td></tr></table></figure></div>

<p>5.利用arange、linspace函数生成数组：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">arange([start,],[stop][,step,],dtype=<span class="literal">None</span>)</span><br><span class="line">linspace(start,stop,num=<span class="number">50</span>,endpoint=<span class="literal">True</span>,restep=<span class="literal">False</span>,dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></div>

<h4 id="二、获取元素："><a href="#二、获取元素：" class="headerlink" title="二、获取元素："></a>二、获取元素：</h4><p>1.使用切片的方式获取对应的数据</p>
<h4 id="三、numpy算术运算："><a href="#三、numpy算术运算：" class="headerlink" title="三、numpy算术运算："></a>三、numpy算术运算：</h4><p>1.np.multiply用于数组或函数对应元素相乘</p>
<p>2.numpy.dot用来计算矩阵的点积</p>
<h4 id="四、更改数组的形状："><a href="#四、更改数组的形状：" class="headerlink" title="四、更改数组的形状："></a>四、更改数组的形状：</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">arr.reshape    <span class="comment"># 重新将向量arr维度进行改变，不改变向量本身</span></span><br><span class="line">arr.resize     <span class="comment"># 重新将向量arr维度进行变换，修改向量本身</span></span><br><span class="line">arr.T          <span class="comment"># 对向量arr进行转置</span></span><br><span class="line">arr.ravel      <span class="comment"># 对向量arr进行展平，即将多维数组变为一维数组，不会产生原来数组的副本</span></span><br><span class="line">arr.flatten    <span class="comment"># 对向量arr进行展平，即将多维数组变成一维数组，返回原数组的副本</span></span><br><span class="line">arr.squeeze    <span class="comment"># 只能对维数为1的维度进行降维</span></span><br><span class="line">arr.transpose  <span class="comment"># 对高维矩阵进行轴对换</span></span><br></pre></td></tr></table></figure></div>

<h4 id="五、合并数组："><a href="#五、合并数组：" class="headerlink" title="五、合并数组："></a>五、合并数组：</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">np.append      <span class="comment"># 内存占用较大</span></span><br><span class="line">np.concatenate <span class="comment"># 没有内存问题</span></span><br><span class="line">np.stack       <span class="comment"># 沿着新的轴加入一系列数组</span></span><br><span class="line">np.hstack      <span class="comment"># 堆栈数组垂直顺序（行）</span></span><br><span class="line">np.vstack      <span class="comment"># 堆栈数组垂直顺序（列）</span></span><br><span class="line">np.dstack      <span class="comment"># 堆栈数组按顺序深入（沿第三维）</span></span><br><span class="line">np.vsplit      <span class="comment"># 将数组分解成垂直的多个子数组列表</span></span><br></pre></td></tr></table></figure></div>

<h4 id="六、广播机制："><a href="#六、广播机制：" class="headerlink" title="六、广播机制："></a>六、广播机制：</h4><p>1.numpy的Universal functions中要求输入的数组shape是一致的，当数组的shape不相等 时，则会使用广播机制 </p>
<p>2.广播机制满足的规则：</p>
<p>（1）让所有输入数组都向其中shape最长的数组看齐，不足的部分则通过在前面加1补齐</p>
<p>（2）输出数组的shape是输入数组shape的各个轴上的最大值 </p>
<p>（3）如果输入数组的某个轴和输出数组的对应轴的长度相同或者某个轴的长度为1时， 这个数组能被用来计算，否则出错 </p>
<p>（4）当输入数组的某个轴的长度为1时，沿着此轴运算时都用（或复制）此轴上的第一组值 </p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习记录2</title>
    <url>/2023/01/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952/</url>
    <content><![CDATA[<h1 id="Pytorch基础"><a href="#Pytorch基础" class="headerlink" title="Pytorch基础"></a>Pytorch基础</h1><h3 id="一、什么是pytorch："><a href="#一、什么是pytorch：" class="headerlink" title="一、什么是pytorch："></a>一、什么是pytorch：</h3><p>1.Pytorch是一个建立在Torch库上的python包</p>
<p>2.pytorch主要的四个组成：</p>
<p>(1).torch：类似于numpy的通用数组库，可以将张量类型转换为torch.cuda.TensorFloat，在GPU上进行计算</p>
<p>(2).torch.autograd：用于构建计算图形并自动获取梯度的包</p>
<p>(3).torch.nn：具有共享层和损失函数的神经网络库</p>
<p>(4).torch.optim：具有通用优化算法的优化包</p>
<h3 id="二、创建Tensor"><a href="#二、创建Tensor" class="headerlink" title="二、创建Tensor:"></a>二、创建Tensor:</h3><ol>
<li></li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">torch.Tensor(*size)     <span class="comment"># 直接从参数构建一个张量，支持list，numpy数组</span></span><br><span class="line">torch.eye(row,column)   <span class="comment"># 创建指定行数，列数的二维单位Tensor</span></span><br><span class="line">torch.linspace(srart,end,steps)   <span class="comment"># 从start到end，均匀切成steps份</span></span><br><span class="line">torch.logspace(start,end,steps)   <span class="comment"># 从10^start，到10^end，均匀切分成steps份</span></span><br><span class="line">torch.rand/randn(*size) <span class="comment"># 生成[0,1)均匀分布/标准正态分布的数据</span></span><br><span class="line">torch.ones(*size)       <span class="comment"># 返回指定shape的张量，元素初始化为1</span></span><br><span class="line">torch.zeros(*size)      <span class="comment"># 返回指定shape的张量，元素初始化为0</span></span><br><span class="line">torch.ones_like(t)      <span class="comment"># 返回与t的shape相同的张量，元素初始化为1</span></span><br><span class="line">torch.zeros_like(t)     <span class="comment"># 返回与他的shape相同的张量，元素初始化为0</span></span><br><span class="line">torch.arange(start,end,step)      <span class="comment"># 在区间[start,end)上以间隔step生成一个序列张量</span></span><br><span class="line">torch.from_Numpy(ndarray)         <span class="comment"># 从ndarray创建一个Tensor</span></span><br></pre></td></tr></table></figure></div>

<h3 id="三、修改tensor的形状："><a href="#三、修改tensor的形状：" class="headerlink" title="三、修改tensor的形状："></a>三、修改tensor的形状：</h3><ol>
<li></li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">size()         <span class="comment"># 返回张量的shape属性值</span></span><br><span class="line">numel(<span class="built_in">input</span>)   <span class="comment"># 计算tensor的元素个数</span></span><br><span class="line">view(*shape)   <span class="comment"># 修改tensor的shape，返回原来的tensor</span></span><br><span class="line">resize         <span class="comment"># 类似于view，但是size超出时会重新分配内存空间</span></span><br><span class="line">item           <span class="comment"># 若tensor为单元素，则返回python的标量</span></span><br><span class="line">unsqueeze      <span class="comment"># 在指定的维度增加一个“1”</span></span><br><span class="line">squeeze        <span class="comment"># 在指定的维度压缩一个“1”</span></span><br></pre></td></tr></table></figure></div>

<h3 id="四、索引操作："><a href="#四、索引操作：" class="headerlink" title="四、索引操作："></a>四、索引操作：</h3><ol>
<li></li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">index_select(<span class="built_in">input</span>,dim,index)   <span class="comment"># 在指定维度上选择一些行或列</span></span><br><span class="line">nonzero(<span class="built_in">input</span>)                  <span class="comment"># 获取非0元素的下标</span></span><br><span class="line">masked_select(<span class="built_in">input</span>,mask)       <span class="comment"># 使用二元值进行选择</span></span><br><span class="line">gather(<span class="built_in">input</span>,dim,index)         <span class="comment"># 在指定维度上选择数据，输出形状与index一致</span></span><br><span class="line">scatter_(<span class="built_in">input</span>,dim,index,src)   <span class="comment"># 为gather的反操作，根据指定索引补充数据</span></span><br></pre></td></tr></table></figure></div>

<h3 id="五、广播机制："><a href="#五、广播机制：" class="headerlink" title="五、广播机制："></a>五、广播机制：</h3><p>1.与numpy中的广播机制类似，pytorch也支持广播机制</p>
<h3 id="六、归并操作："><a href="#六、归并操作：" class="headerlink" title="六、归并操作："></a>六、归并操作：</h3><ol>
<li></li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">cumprod(t,axis)     <span class="comment"># 在指定维度对t进行累积</span></span><br><span class="line">cumsum              <span class="comment"># 在指定维度对t进行累加</span></span><br><span class="line">dist(a,b,p=<span class="number">2</span>)       <span class="comment"># 返回a,b之间的p阶范数</span></span><br><span class="line">mean/median         <span class="comment"># 均值/中位数</span></span><br><span class="line">std/var             <span class="comment"># 标准差/方差</span></span><br><span class="line">norm(t,p=<span class="number">2</span>)         <span class="comment"># 返回t的p阶范数</span></span><br><span class="line">prod(t)/<span class="built_in">sum</span>(t)      <span class="comment"># 返回t的所有元素的积/和</span></span><br></pre></td></tr></table></figure></div>

<h3 id="七、比较操作："><a href="#七、比较操作：" class="headerlink" title="七、比较操作："></a>七、比较操作：</h3><ol>
<li></li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">eq                  <span class="comment"># 比较tensor是否相等，支持broadcast</span></span><br><span class="line">equal               <span class="comment"># 比较tensor是否有相同的shape与值</span></span><br><span class="line">ge/le/gt/lt         <span class="comment"># 大于/小于比较/大于等于/小于等于比较</span></span><br><span class="line"><span class="built_in">max</span>/<span class="built_in">min</span>(t,axis)     <span class="comment"># 返回最值，若指定axis，则额外返回下标</span></span><br><span class="line">top(t,k,axis)       <span class="comment"># 在指定的axis维上取最高的K个值</span></span><br></pre></td></tr></table></figure></div>

<h3 id="八、矩阵操作："><a href="#八、矩阵操作：" class="headerlink" title="八、矩阵操作："></a>八、矩阵操作：</h3><ol>
<li></li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">dot(t1,t2)         <span class="comment"># 计算张量（1D）的内积或点积</span></span><br><span class="line">mm(mat1,mat2)/bmm(batch1,batch2)   <span class="comment"># 计算矩阵乘法/含batch的3D矩阵乘法</span></span><br><span class="line">mv(t1,v1)          <span class="comment"># 计算矩阵与向量乘法</span></span><br><span class="line">t                  <span class="comment"># 计算转置</span></span><br><span class="line">svd(t)             <span class="comment"># 计算t的SVD分解</span></span><br></pre></td></tr></table></figure></div>]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录7</title>
    <url>/2023/01/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%957/</url>
    <content><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h4 id="一、神经网络模型："><a href="#一、神经网络模型：" class="headerlink" title="一、神经网络模型："></a>一、神经网络模型：</h4><p>1.神经网络是由具有适应性的简单单元组成的广泛并行互连的网络</p>
<p>2.神经网络是一个很大的学科领域，在机器学习中研究的是神经网络学习，亦称为连接主义学习</p>
<p>3.神经网络学到的知识蕴含在连接权重和阈值</p>
<p>4.神经网络需要激活函数，如sigmoid函数</p>
<p>5.前馈网络：神经元之间不存在同层连接也不存在跨层连接</p>
<h4 id="二、万有逼近能力："><a href="#二、万有逼近能力：" class="headerlink" title="二、万有逼近能力："></a>二、万有逼近能力：</h4><p>1.多层前馈神经网络有强大的表示能力</p>
<p>2.仅需一个包含足够多神经元的隐层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数</p>
<h4 id="三、BP算法（误差反向传播算法）："><a href="#三、BP算法（误差反向传播算法）：" class="headerlink" title="三、BP算法（误差反向传播算法）："></a>三、BP算法（误差反向传播算法）：</h4><p>1.BP是一个迭代学习算法，在迭代的每一轮中采用广义感知机学习规则</p>
<p>2.BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整</p>
<p>3.学习率不能太大，也不能太小。取值在0-1之间</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录8</title>
    <url>/2023/02/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%958/</url>
    <content><![CDATA[<h1 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h1><h3 id="一、贝叶斯决策论："><a href="#一、贝叶斯决策论：" class="headerlink" title="一、贝叶斯决策论："></a>一、贝叶斯决策论：</h3><p>1.概率框架下实施决策的基本理论</p>
<p>2.h*称为贝叶斯最优分类器，其总体风险称为贝叶斯风险</p>
<p>反映了学习性能的理论上限</p>
<h3 id="二、生成式和判别式模型："><a href="#二、生成式和判别式模型：" class="headerlink" title="二、生成式和判别式模型："></a>二、生成式和判别式模型：</h3><p>1.判别式：直接对条件概率分布建模</p>
<p>2.生成式：先对联合概率分布建模，再由此得到条件概率</p>
<h3 id="三、贝叶斯分类器和贝叶斯学习："><a href="#三、贝叶斯分类器和贝叶斯学习：" class="headerlink" title="三、贝叶斯分类器和贝叶斯学习："></a>三、贝叶斯分类器和贝叶斯学习：</h3><p>1.频率主义：统计学习，点估计</p>
<p>2.贝叶斯主义：贝叶斯学习，分布估计</p>
<h3 id="四、极大似然估计："><a href="#四、极大似然估计：" class="headerlink" title="四、极大似然估计："></a>四、极大似然估计：</h3><p>1.先假设某种概率分布形式，再基于训练样例对参数进行估计</p>
<h3 id="五、朴素贝叶斯分类器："><a href="#五、朴素贝叶斯分类器：" class="headerlink" title="五、朴素贝叶斯分类器："></a>五、朴素贝叶斯分类器：</h3><p>1.主要的障碍：所有属性上的联合概率难以从有限的训练样本获得、组合爆炸、样本稀疏</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习记录9</title>
    <url>/2023/02/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%959/</url>
    <content><![CDATA[<h1 id="集成学习和聚类"><a href="#集成学习和聚类" class="headerlink" title="集成学习和聚类"></a>集成学习和聚类</h1><h3 id="一、集成学习："><a href="#一、集成学习：" class="headerlink" title="一、集成学习："></a>一、集成学习：</h3><p>1.使用多个模型来学习</p>
<p>分为同质集成学习和异质集成学习</p>
<h3 id="二、好而不同："><a href="#二、好而不同：" class="headerlink" title="二、好而不同："></a>二、好而不同：</h3><p>1.个体学习器“好而不同”</p>
<p>2.误差-分歧分解</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://mystorage9.oss-cn-hangzhou.aliyuncs.com/img/%E8%AF%AF%E5%B7%AE-%E5%88%86%E6%AD%A7%E5%88%86%E8%A7%A3.png"
                     
                ></p>
<h3 id="三、两类常用的集成学习方法："><a href="#三、两类常用的集成学习方法：" class="headerlink" title="三、两类常用的集成学习方法："></a>三、两类常用的集成学习方法：</h3><p>1.序列化方法：boosting</p>
<p>2.并行化方法：bagging</p>
<h3 id="四、Boosting："><a href="#四、Boosting：" class="headerlink" title="四、Boosting："></a>四、Boosting：</h3><p>1.残差最小化，残差逼近</p>
<p>2.序列化模型</p>
<h3 id="五、Bagging："><a href="#五、Bagging：" class="headerlink" title="五、Bagging："></a>五、Bagging：</h3><p>1.使用可重复采样</p>
<p>2.投票做分类，平均做回归</p>
<h3 id="六、多样性度量："><a href="#六、多样性度量：" class="headerlink" title="六、多样性度量："></a>六、多样性度量：</h3><p>1.多样性是集成学习的关键</p>
<p>2.多样性度量：一般通过两分类器的预测结果列联表定义</p>
<p>如不合度量、相关系数、Q-统计量、K-统计量</p>
<h3 id="七、聚类："><a href="#七、聚类：" class="headerlink" title="七、聚类："></a>七、聚类：</h3><p>1.聚类是“无监督学习”中研究最多、应用最广的任务</p>
<p>2.聚类的目标：将数据样本划分为若干个通常不相交的“簇”</p>
<p>既可以作为一个单独任务（用于寻找数据内在的分布结构），也可以作为分类等其他学习任务的前驱过程</p>
<h3 id="八、聚类方法概述："><a href="#八、聚类方法概述：" class="headerlink" title="八、聚类方法概述："></a>八、聚类方法概述：</h3><p>1.聚类的好坏不存在绝对标准</p>
<p>2.在聚类中，总能找到一个新的“标准”，使得以往的算法对它无能为力</p>
<p>3.常用的聚类方法：</p>
<p>（1）原型聚类：亦称为“基于原型的聚类”</p>
<p>​	 假设：聚类结构能通过一组原型刻画</p>
<p>​	 过程：先对原型初始化，然后对原型进行迭代更新求解</p>
<p>​	 代表：K均值聚类，学习向量量化（LVQ），高斯混合聚类</p>
<p>（2）密度聚类：亦称为“基于密度的聚类”</p>
<p>​	 假设：聚类结构能通过样本分布的紧密程度确定</p>
<p>​	 过程：从样本密度的角度来考察样本之间的可连接性，并基于可连接性样本不断扩展聚类簇</p>
<p>​	 代表：DBSCAN，OPTICS，DENCLUE</p>
<p>（3）层次聚类：</p>
<p>​	 假设：能够产生不同粒度的聚类结果</p>
<p>​	 过程：在不同层次对数据集进行划分，从而形成树形的聚类结构</p>
<p>​	 代表：AGNES（自底向上），DIANA（自顶向下）</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
